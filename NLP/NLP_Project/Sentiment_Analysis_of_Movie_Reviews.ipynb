{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Preprocessing:**\n",
        "\n",
        "1. Tokenization\n",
        "2. Stemming\n",
        "3. Lemmatization"
      ],
      "metadata": {
        "id": "YOldFiXjI1Te"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer"
      ],
      "metadata": {
        "id": "0EZuCKwnI4Gd"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download necessary NLTK data\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cKQ5FFReKqyN",
        "outputId": "1c75fdb3-6008-437a-800e-51c9a2189264"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = set(stopwords.words('english'))\n",
        "stemmer=PorterStemmer()\n",
        "lemmatizer=WordNetLemmatizer()"
      ],
      "metadata": {
        "id": "ERjQBk_3Kvwx"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = [\n",
        "    \"This movie was fantastic! I loved every minute of it.\",\n",
        "    \"Terrible movie, complete waste of time.\",\n",
        "    \"It was an okay movie, not bad but not great either.\",\n",
        "    \"Amazing film! Definitely recommend watching it.\",\n",
        "    \"Awful film, I do not recommend it to anyone.\",\n",
        "    \"An absolute masterpiece, I was blown away!\",\n",
        "    \"One of the worst movies I’ve ever seen.\",\n",
        "    \"Great performances and an engaging story.\",\n",
        "    \"Boring, predictable, and poorly written.\",\n",
        "    \"Enjoyable film with a strong message.\",\n",
        "    \"I wouldn't watch this again, not worth the time.\",\n",
        "    \"Outstanding cinematography but the plot was weak.\",\n",
        "    \"A must-watch for fans of the genre.\",\n",
        "    \"Disappointing. I expected much more from it.\",\n",
        "    \"Full of heart and emotion, a beautiful film.\",\n",
        "    \"I fell asleep halfway through, it was that dull.\",\n",
        "    \"A delightful family movie that everyone can enjoy.\",\n",
        "    \"Poor acting and terrible special effects.\",\n",
        "    \"A thought-provoking film that stays with you.\",\n",
        "    \"Not my cup of tea, but some might enjoy it.\",\n",
        "    \"I thoroughly enjoyed it from start to finish!\",\n",
        "    \"Complete rubbish, couldn't wait for it to end.\",\n",
        "    \"A fun ride with plenty of laughs.\",\n",
        "    \"Too long and lacked direction.\",\n",
        "    \"Great soundtrack but the story didn’t hold up.\",\n",
        "    \"A charming movie with lovable characters.\",\n",
        "    \"Not as good as I had hoped, very disappointing.\",\n",
        "    \"Well-executed with a unique premise.\",\n",
        "    \"I regret paying to see this in the theater.\",\n",
        "    \"Top-notch action scenes, but not much else.\",\n",
        "    \"Beautifully shot but ultimately forgettable.\",\n",
        "    \"One of my favorite movies this year!\",\n",
        "    \"I couldn't make it past the first 20 minutes.\",\n",
        "    \"A compelling drama with outstanding performances.\",\n",
        "    \"Too confusing and hard to follow.\",\n",
        "    \"A perfect film for a cozy night in.\",\n",
        "    \"Ridiculous plot but entertaining nonetheless.\",\n",
        "    \"A refreshing take on the superhero genre.\",\n",
        "    \"This is one of those movies you watch once and forget.\",\n",
        "    \"A rollercoaster of emotions from start to finish.\",\n",
        "    \"Didn't live up to the hype, very overrated.\",\n",
        "    \"I loved it! Will definitely be watching again.\",\n",
        "    \"Waste of time, don’t bother.\",\n",
        "    \"A truly captivating film experience.\",\n",
        "    \"Poor dialogue and a lackluster story.\",\n",
        "    \"A solid entry in the franchise.\",\n",
        "    \"Utterly terrible in every aspect.\",\n",
        "    \"I highly recommend this to everyone.\",\n",
        "    \"Barely watchable, very poorly done.\",\n",
        "    \"One of the best movies I've seen in a while.\",\n",
        "    \"Uninspired and boring from beginning to end.\",\n",
        "    \"The acting saved this otherwise mediocre film.\",\n",
        "    \"Just okay, nothing special.\",\n",
        "    \"I can't believe this movie got such good reviews.\",\n",
        "    \"Brilliant! A fantastic watch.\",\n",
        "    \"Complete garbage, don’t waste your money.\",\n",
        "    \"A film that truly speaks to the heart.\",\n",
        "    \"Just when I thought it couldn’t get worse, it did.\",\n",
        "    \"A powerful and moving story.\",\n",
        "    \"Could have been so much better.\",\n",
        "    \"An instant classic, loved every second.\",\n",
        "    \"A dull film with very little excitement.\",\n",
        "    \"A true work of art, visually stunning.\",\n",
        "    \"One of the worst films I’ve ever seen.\",\n",
        "    \"A beautifully crafted movie with great depth.\",\n",
        "    \"Absolutely awful, avoid at all costs.\",\n",
        "    \"A movie that will inspire generations to come.\",\n",
        "    \"Completely forgettable, not worth your time.\",\n",
        "    \"An exciting and thrilling adventure.\",\n",
        "    \"I’ve never seen a movie this bad.\",\n",
        "    \"A touching story with real emotional depth.\",\n",
        "    \"Predictable and lacking originality.\",\n",
        "    \"It was good but not as great as people say.\",\n",
        "    \"A disappointing sequel to a great first movie.\",\n",
        "    \"An uplifting and feel-good film.\",\n",
        "    \"A total disaster, hard to watch.\",\n",
        "    \"It was a decent movie, but not amazing.\",\n",
        "    \"I hated every minute of this movie.\",\n",
        "    \"Surprisingly good, I didn’t expect to like it.\",\n",
        "    \"Poorly acted, poorly written, just bad.\",\n",
        "    \"A standout film with great performances.\",\n",
        "    \"I wish I could get those two hours back.\",\n",
        "    \"Loved the cinematography, but the rest was meh.\",\n",
        "    \"A perfect blend of humor and drama.\",\n",
        "    \"What a waste of a great cast.\",\n",
        "    \"A heartwarming story that left me in tears.\",\n",
        "    \"Don't bother with this one, it's not worth it.\",\n",
        "    \"An impressive and thrilling action movie.\",\n",
        "    \"I didn't enjoy it, not my type of movie.\",\n",
        "    \"A must-see for any movie fan.\"\n",
        "]\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "QU-InqRBK75S"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(text):\n",
        "    tokens = word_tokenize(text.lower())\n",
        "    tokens = [word for word in tokens if word.isalnum() and word not in stop_words]\n",
        "    stemmed = [stemmer.stem(word) for word in tokens]\n",
        "    lemmatized = [lemmatizer.lemmatize(word) for word in stemmed]\n",
        "    return ' '.join(lemmatized)\n",
        "\n",
        "preprocessed_data = [preprocess(review) for review in data]\n",
        "print(\"Preprocessed Data:\\n\", preprocessed_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wds7jjfzLzEx",
        "outputId": "da31af80-598e-4278-fef4-e21a74b0c16a"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preprocessed Data:\n",
            " ['movi fantast love everi minut', 'terribl movi complet wast time', 'okay movi bad great either', 'amaz film definit recommend watch', 'aw film recommend anyon', 'absolut masterpiec blown away', 'one worst movi ever seen', 'great perform engag stori', 'bore predict poorli written', 'enjoy film strong messag', 'would watch worth time', 'outstand cinematographi plot weak', 'fan genr', 'disappoint expect much', 'full heart emot beauti film', 'fell asleep halfway dull', 'delight famili movi everyon enjoy', 'poor act terribl special effect', 'film stay', 'cup tea might enjoy', 'thoroughli enjoy start finish', 'complet rubbish could wait end', 'fun ride plenti laugh', 'long lack direct', 'great soundtrack stori hold', 'charm movi lovabl charact', 'good hope disappoint', 'uniqu premis', 'regret pay see theater', 'action scene much el', 'beauti shot ultim forgett', 'one favorit movi year', 'could make past first 20 minut', 'compel drama outstand perform', 'confus hard follow', 'perfect film cozi night', 'ridicul plot entertain nonetheless', 'refresh take superhero genr', 'one movi watch forget', 'rollercoast emot start finish', 'live hype overr', 'love definit watch', 'wast time bother', 'truli captiv film experi', 'poor dialogu lacklust stori', 'solid entri franchis', 'utterli terribl everi aspect', 'highli recommend everyon', 'bare watchabl poorli done', 'one best movi seen', 'uninspir bore begin end', 'act save otherwis mediocr film', 'okay noth special', 'ca believ movi got good review', 'brilliant fantast watch', 'complet garbag wast money', 'film truli speak heart', 'thought get wors', 'power move stori', 'could much better', 'instant classic love everi second', 'dull film littl excit', 'true work art visual stun', 'one worst film ever seen', 'beauti craft movi great depth', 'absolut aw avoid cost', 'movi inspir gener come', 'complet forgett worth time', 'excit thrill adventur', 'never seen movi bad', 'touch stori real emot depth', 'predict lack origin', 'good great peopl say', 'disappoint sequel great first movi', 'uplift film', 'total disast hard watch', 'decent movi amaz', 'hate everi minut movi', 'surprisingli good expect like', 'poorli act poorli written bad', 'standout film great perform', 'wish could get two hour back', 'love cinematographi rest meh', 'perfect blend humor drama', 'wast great cast', 'heartwarm stori left tear', 'bother one worth', 'impress thrill action movi', 'enjoy type movi', 'movi fan']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Vectorization Techniques:**\n",
        "\n",
        "Now, we'll convert the preprocessed text into numerical vectors using the following methods:\n",
        "\n",
        "1. One Hot Encoding\n",
        "2. Bag of Words\n",
        "3. TF-IDF\n",
        "4. CBOW (Word2Vec)"
      ],
      "metadata": {
        "id": "f9LPTzzhNCjC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**One Hot Encoding**"
      ],
      "metadata": {
        "id": "OTpa-jiBNd4g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import numpy as np\n",
        "\n",
        "onehot_encoder = OneHotEncoder(sparse=False)\n",
        "onehot_encoded = onehot_encoder.fit_transform(np.array(preprocessed_data).reshape(-1, 1))\n",
        "print(\"One Hot Encoded Data:\\n\", onehot_encoded)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oQlvawq7NgIx",
        "outputId": "d43e0942-0acf-4b33-fd5d-be398c212b48"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "One Hot Encoded Data:\n",
            " [[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Bag of Words**"
      ],
      "metadata": {
        "id": "_r4lQw4IN_-T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "bow_vectorizer = CountVectorizer()\n",
        "X_bow = bow_vectorizer.fit_transform(preprocessed_data)\n",
        "print(\"Bag of Words Representation:\\n\", X_bow.toarray())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "__iBQUMYODSx",
        "outputId": "c8e31d94-53c8-4663-fe95-01215a4d5d13"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bag of Words Representation:\n",
            " [[0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " ...\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TF-IDF**"
      ],
      "metadata": {
        "id": "rnluMnqWONUx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "X_tfidf = tfidf_vectorizer.fit_transform(preprocessed_data)\n",
        "print(\"TF-IDF Representation:\\n\", X_tfidf.toarray())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yn80sm6nOP2J",
        "outputId": "525423ef-4511-49cf-d5dc-95d85dfe439d"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TF-IDF Representation:\n",
            " [[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Continuous Bag of Words (CBOW)**"
      ],
      "metadata": {
        "id": "ksLul2kyOVb0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "tokenized_data = [review.split() for review in preprocessed_data]\n",
        "\n",
        "# Train Word2Vec model (CBOW model: sg=0)\n",
        "cbow_model = Word2Vec(sentences=tokenized_data, vector_size=100, window=5, min_count=1, sg=0)\n",
        "\n",
        "# To represent each sentence, average the word vectors of all words in the sentence\n",
        "def get_sentence_vector(sentence, model):\n",
        "    words = sentence.split()\n",
        "    word_vectors = [model.wv[word] for word in words if word in model.wv]\n",
        "    return np.mean(word_vectors, axis=0)\n",
        "\n",
        "X_cbow = np.array([get_sentence_vector(sentence, cbow_model) for sentence in preprocessed_data])\n",
        "print(\"CBOW Word Vectors:\\n\", X_cbow)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fkyHt84IOX5K",
        "outputId": "20383946-39fc-48f3-b6a7-dd9cd36948fe"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CBOW Word Vectors:\n",
            " [[-6.3912629e-04  3.6210469e-03  1.8243576e-04 ... -1.7634958e-03\n",
            "  -1.5856440e-03  4.4760453e-03]\n",
            " [-5.0168391e-03  1.7704182e-04  2.2441749e-03 ...  3.4834102e-03\n",
            "   4.7718617e-03  4.1062124e-03]\n",
            " [ 1.3673218e-03 -7.6835445e-04  1.9242356e-03 ...  6.0570234e-04\n",
            "   2.1600276e-03  2.7421662e-03]\n",
            " ...\n",
            " [-1.8390713e-03 -3.9312951e-03  2.9161647e-03 ... -4.6399934e-04\n",
            "  -4.7269980e-03  3.7453952e-04]\n",
            " [ 2.4709068e-03 -4.2579565e-03 -8.6342916e-04 ... -3.3811554e-03\n",
            "   2.6316105e-03  5.7820170e-03]\n",
            " [ 2.3844498e-03  5.0336327e-03 -9.4403536e-04 ...  8.7188859e-04\n",
            "  -2.8905575e-05  2.7699713e-03]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Train a Model and Compare**\n",
        "\n",
        "We can train a simple logistic regression model and compare the performance of each vectorization technique"
      ],
      "metadata": {
        "id": "e1TBzQcsOlTU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Using One Hot Encoder\n",
        "\n",
        "# Train-test split for One Hot Encoded\n",
        "X_train, X_test, y_train, y_test = train_test_split(onehot_encoded, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train logistic regression\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy using OneHotEncoder: {accuracy}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VBlN3_sfOrKV",
        "outputId": "289939ce-6997-4627-b579-56b31bd2f996"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy using OneHotEncoder: 0.3888888888888889\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Using BOW\n",
        "\n",
        "# Train-test split for BOW\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_bow, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train logistic regression\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy using BOW: {accuracy}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cmPXcqjjPKZK",
        "outputId": "3092b0de-85e4-4c46-8bf3-4d9f0737c6e7"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy using BOW: 0.3888888888888889\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Using CBOW\n",
        "\n",
        "# Train-test split for CBOW\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_cbow, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train logistic regression\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy using CBOW: {accuracy}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sxd_2i3TP5da",
        "outputId": "9d26ca9c-bacd-4faf-9721-ba9aab23181f"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy using CBOW: 0.3888888888888889\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Example: Using TF-IDF (You can repeat the same process for OneHot, BoW, and CBOW)\n",
        "\n",
        "# Train-test split for TF-IDF\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_tfidf, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train logistic regression\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy using TF-IDF: {accuracy}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZFO77gFoP5js",
        "outputId": "c3191476-54fa-47c3-c9e3-c264cc3d68c3"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy using TF-IDF: 0.4444444444444444\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Testing the Model with a New Sentence**"
      ],
      "metadata": {
        "id": "TnZFNk4JdHYH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample new sentence for testing\n",
        "new_sentence = \"This film was absolutely fantastic!\"\n",
        "\n",
        "# Preprocess the new sentence\n",
        "processed_sentence = preprocess(new_sentence)\n",
        "\n",
        "# Transform the processed sentence using the same TF-IDF vectorizer\n",
        "# Note: Make sure to use the same vectorizer you fitted on the training data\n",
        "new_sentence_tfidf = tfidf_vectorizer.transform([processed_sentence])\n",
        "\n",
        "# Make prediction using the trained model\n",
        "prediction = model.predict(new_sentence_tfidf)\n",
        "\n",
        "# Output the prediction\n",
        "print(\"Predicted Sentiment:\", \"Positive\" if prediction[0] == 1 else \"Negative\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XZ2HkIEudGu8",
        "outputId": "a29fdaee-dcc7-4759-ddd9-242af6cb23c8"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted Sentiment: Positive\n"
          ]
        }
      ]
    }
  ]
}